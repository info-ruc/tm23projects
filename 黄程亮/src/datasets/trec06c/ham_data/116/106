Received: from turing.cs.hmc.edu (turing.cs.hmc.edu [134.173.42.99])
	by sundial.cs.cornell.edu (8.11.7-20031020/8.11.7/M-3.22) with ESMTP id k227NEt04970
	for <egs+summary@cs.cornell.edu>; Thu, 2 Mar 2006 02:23:14 -0500 (EST)
Received: by turing.cs.hmc.edu (Postfix, from userid 34382)
	id 7DF275326D; Wed,  1 Mar 2006 23:23:08 -0800 (PST)
Date: Wed, 1 Mar 2006 23:23:08 -0800
From: Victoria Krafft <vmk@cs.hmc.edu>
To: egs+summary@cs.cornell.edu
Subject: PAPER 11
Message-ID: <20060302072308.GB9120@cs.hmc.edu>
Mime-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.4.2.1i

The current DNS system has been around for more than 20 years.  The
basic design is tree-based with heavy caching. It relies on a single
set of authoritative servers, which branch out rapidly to smaller
servers for individual domains and subdomains.  By caching data along
the tree, the system avoids overwhelming the root servers.

The paper by Cox et al. proposes a new distributed DNS system, called
DDNS.  It would be implemented on top of a Chord p2p network, and use
DNSSEC, which uses digital signatures to authenticate the domain
records.  They investigate this idea, and end up deciding that the
current system should be retained.  The new system offers more
stability for individual domains, but it has longer latencies, and
does not allow servers to provide dynamic name resolution, which is
commonly done for a variety of applications.

In addition to these problems, I have to question the logic behind
their original motivation. The main problem identified, the inability
of users to run their own nameservers.This is not a property of the
existing DNS architecture.  It's a property of a poor interface to the
common software for managing DNS, BIND.  It is currently remedied by
companies who, for a small monthly fee, will provide name service for
a domain.  Without this problem, we have no motivation for switching
to a p2p system.

CoDoNS is another distributed DNS system, which uses Beehive for its
underlying network topology.  Like DDNS, it relies on DNSSEC to
authenticate record in its system.  With its proactive caching
strategy, Beehive provides much faster lookup times, and handles flash
crowds almost as well as the existing system.  It fixes many of the
problems current DNS servers face, such as load imbalances and DDOS
attacks.  This system is backwards compatible with the existing DNS
network, so organizations can transition at any time.  The free rider
problem is, for the most part, glossed over, with a note that large
organizations will pay for the DNS servers.

While this system is quite impressive, they gloss over one potential
problem when transitioning: dynamically generated DNS records.  These
have a variety of uses, such as sharing load between servers, and they
work by providing different copies of a DNS record to different
sections of the network.  In many cases, there is no good way to
offload the decision onto the DNS client; people will misconfigure
those, or hack the system so that they get the optimal behavior,
regardless of what this does to other users.  Because this decision
can't be offloaded to a client machine, either CoDoNS will have to
process these, or they will have to send all the traffic to some other
server, which then provides dynamically generated records.  While
CoDoNS is running with records generated by the old DNS, this could
cause serious problems: If a company normally splits the load between
100 servers, and CoDoNS is used by half the internet, then one of the
machines will be overwhelmed, while the rest see less traffic.

-- Victoria Krafft

