Received: from bach.graphics.cornell.edu (bach.graphics.cornell.edu [128.84.247.50])
	by sundial.cs.cornell.edu (8.11.3/8.11.3/M-3.10) with ESMTP id gACH1DQ25919
	for <egs@cs.cornell.edu>; Tue, 12 Nov 2002 12:01:13 -0500 (EST)
Received: from envy.graphics.cornell.edu (envy.graphics.cornell.edu [128.84.247.206])
	by bach.graphics.cornell.edu (8.12.1/8.12.1) with ESMTP id gACH170k067949
	for <egs@cs.cornell.edu>; Tue, 12 Nov 2002 12:01:07 -0500 (EST)
Date: Tue, 12 Nov 2002 12:00:35 -0500 (EST)
From: Adam Kravetz <adam@graphics.cornell.edu>
To: egs@CS.Cornell.EDU
Subject: 615 Paper 63
Message-ID: <Pine.BSF.4.44.0211121102480.94002-100000@envy.graphics.cornell.edu>
MIME-Version: 1.0
Content-Type: TEXT/PLAIN; charset=US-ASCII

The Small-World Phenomenon:

6-degrees of separation, a common party game, and social theory applied to
computer networks and connectivity.  This paper gives us three theorems
that predict expected delivery time, lower bound and upper bounds all
based on the 6-degree ideas.

Performance:

This paper takes a look at Freenet and Gnutella as case studies.  This
paper points out that performance matters in P2P systems, Moore's law
doesn't apply to bandwidth so waiting for faster modems and connections
isn't going to be fruitful.  Maximizing efficiency on a network is the
best way to improve transport, scalability and other issues associated w/
P2P.  In the case Study of Freenet it is found that pathlength scales
logarithmically w/ the size of the network so in theory it scales well.
Gnutella on the other hand scales not w/ path length but by the number of
messages sent giving it a poor scalability due to the fact that as
networks get bigger more and more messages are propogated per search.
Gnutella scales linearly (bad) but a suggested hierarchical model could
help.


Measurement of P2P:

This paper takes a look at Napster and Gnutella (structured centralized
networks vs. unstructured decenteralized).  They conclude from their
studies that they would like to see un-shared responsibilities across a
network.  I think this suggestion is good, but should be used as a
parameter of an adaptive system.  Nodes are allowed to grow or fall in
importance depending on resources available at any given time.  They also
make the important observation that people are willing to participate in a
P2P system and this is not necessarily true.  Generally people want to
participate only if they feel it is to there advantage.


Unstructured P2P systems can work, they however need highly adaptable,
smarter and better constructed algorithms to control them.  The design of
an unstructured P2P needs to be topologically designed better.  I think
exploiting the small-world network discovery and trying to distributedly
build topologies that use this information is important.  Moving away from
a metric where a "hop" is a link from node A to node B in a P2P system
instead of the 10 router hops that it actually is, is a good first step.
Using a set of close neighbors (topologically on the internet) may help
keep network load down, and allow for greater scalability.  This however
falls pray to the problem that information clustering will happen (your
local subset of nodes to ask has roughly similar data) and that a random
outside link metric (which is what gives 6-degree type theories their
successes) would need to be introduced.

To obtain a good, scalable, reliable P2P network a good first idea would
be to try and build extremely lightweight, highly adaptable link systems
which attempted to have high path optimality.  A second part of a P2P
system should try to adapt by the availability of resources in the
network, promoting links to higher status based on their ability to carry
load (bandwidth or processing power or battery life).  Finally a third
part of a P2P system would focus on fault tolerance.  This sort of hybrid,
multi-pronged, layered approach to P2P systems would hopefully provide a
more globally scalable system.



