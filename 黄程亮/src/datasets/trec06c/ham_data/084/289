Received: from travelers.mail.cornell.edu (travelers.mail.cornell.edu [132.236.56.13])
	by sundial.cs.cornell.edu (8.11.3/8.11.3/M-3.10) with ESMTP id gACJcPQ14154
	for <egs@cs.cornell.edu>; Tue, 12 Nov 2002 14:38:26 -0500 (EST)
Received: by travelers.mail.cornell.edu (8.9.3/8.9.3) id OAA09836;
	Tue, 12 Nov 2002 14:38:15 -0500 (EST)
Date: Tue, 12 Nov 2002 14:38:15 -0500 (EST)
From: ks238@cornell.edu
Message-Id: <200211121938.OAA09836@travelers.mail.cornell.edu>
To: egs@CS.Cornell.EDU
Errors-To: ks238@cornell.edu
Reply-To: ks238@cornell.edu
MIME-Version: 1.0
Content-Type: text/plain
Content-Transfer-Encoding: 7bit
X-Mailer: IMP/PHP3 Imap webMail Program 2.0.9
Sender: ks238@cornell.edu
X-Originating-IP: 128.84.99.155
Subject: 615 Paper 63

The focus of these three papers is on decentralized P2P 
systems and searching through them. 

We first start with Kleinberg’s study of the “Small 
World Phenomenon” which is an algorithmic approach to 
routing in a decentralized P2P system. This notion, 
which was founded my Milgram after conducting a series 
of societal experiments led him to conclude that the 
average distance (measured in the number of 
intermediate people between a starting person and a 
target node) is approximately 5.5. Kleinberg uses this 
notion and applies it to the study of decentralized 
networks and their inherent structure. He argues that 
while searching in a network that exhibits small world 
properties one must look for that node which exhibits 
the largest jump from one cluster of nodes to another. 
In other words, the person who almost serves as 
a “gateway” to an alternate cluster of people in an 
area that is close to the target. When such nodes can 
be contacted, it is possible to have (log n)^2 nodes in 
each route, where n is the total number of nodes in the 
network. This is a very efficient runtime that is 
achieved and proved in the paper.

In the next paper we see an evaluation between Gnutella 
and Freenet. In Freenet we see a system which assumes a 
connected network, whereby, traversing certain nodes in 
the graph can and will eventually allow us to reach the 
target node. Also, it assumes that short routes exist 
between two peers so that they can communicate with a 
relative small number of hops. Because Freenet exhibits 
the small world properties discussed by Kleinberg, the 
number of hops in routes is about the same. The second 
case that is studied is that of Gnutella, which doesn’t 
assume a small world. The key point of Gnutella is that 
it employs a broadcasting scheme which “saturates” the 
network with requests in an effort to find the shortest 
route. Each node tries to have three more simultaneous 
links that it broadcasts to and through iterative 
broadcasting at each hop, the target is eventually 
found. Gnutella is good in finding optimal paths 
however is weak since it costs so much to reach the 
results it does.

The final paper is a measurement study of the Napster 
and Gnutella file sharing systems and the 
characteristics which reduce the performance of them. 
In the paper, the two systems were crawled with unique 
crawlers for both systems. Then based on these crawls, 
latency measurements, lifetime measurements and 
bottleneck bandwidth measurements were recorded. From 
these measurements a couple of contributions were made. 
First, they concluded that more people were serving as 
clients rather than servers. In other words, there was 
a larger number of peers that were downloading files 
rather than sharing files. In addition, another problem 
was the level of bandwidth that certain nodes exhibited 
that could potentially bottleneck a file transfer. To 
conclude, the primary contribution of this paper was to 
suggest the relevance of individual node properties in 
the actual performance of a P2P system. 

These papers touch on some very interesting and hot 
areas of current research. Many of the questions that 
should be addressed about such systems is security. P2P 
systems have an immense number of possible security 
breaches since your network’s performance is contingent 
on the reliability of other nodes. Also, an interesting 
topic that could also be discussed is using certain 
caching schemes that based on frequency of requests 
could mitigate the costs associated with long distant 
queries that are inherently expensive.

