{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c63427-7086-459c-8342-9e9ca87e9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import transformers\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer,BertConfig,AdamW,BertModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7be0e1-cb7a-4593-8e0f-e5fb8d03ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5942\n",
      "743\n",
      "743\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "Data_path = \"./JDcontents_vivo_Cleaned_data.csv\"\n",
    "Totle_data = pd.read_csv(Data_path)\n",
    " \n",
    "train_dataset,temp_data =  train_test_split(Totle_data,test_size=0.2)\n",
    "validate_dataset, test_dataset = train_test_split(temp_data,test_size=0.5)\n",
    "print(len(train_dataset))\n",
    "print(len(validate_dataset))\n",
    "print(len(test_dataset))\n",
    " \n",
    "#设置保存路径\n",
    "train_data_path=\"./Train.csv\"\n",
    "dev_data_path = \"./Dev.csv\" \n",
    "test_data_path=\"./Test.csv\"\n",
    " \n",
    "#index参数设置为False表示不保存行索引,header设置为False表示不保存列索引\n",
    "train_dataset.to_csv(train_data_path,index=False,header=True)\n",
    "validate_dataset.to_csv(dev_data_path ,index=False,header=True)\n",
    "test_dataset.to_csv(test_data_path,index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63924cc-bd4c-461a-8121-31891fbf70b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>一直 蓝厂 手机 妈妈 四年 该换 正好 刚出 配置 更好 更新 果断 入手 买 礼盒装 几...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>款 手机 颜色 太漂亮 玻璃 后盖绝 绝子 颜控 表示 喜欢 拍照 好看 电池 耐用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>刚刚 买来 一个月 不到 电话 接听 不到 一天 几十个 电话 进来 手机 重要 功能 问题...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>运行 速度 真的 不要 太爽 使用 起来 不会 卡顿 之前 明显 不够 外形 外观 真的 很...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>辣鸡 客服 刚买 手机 降价 补 差价</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>0</td>\n",
       "      <td>最差 手机 说 手机 原来 手机 网络 差 要死</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>4</td>\n",
       "      <td>拍照 效果 刚回来 试 一下 拍照 自带 美妆 挺不错 外形 外观 外观 好看 感觉 挺值 价</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>4</td>\n",
       "      <td>外形 外观 外观 很漂亮 喜欢 屏幕 音效 生音 很大 拍照 效果 杠杠 运行 速度 很快 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5940</th>\n",
       "      <td>2</td>\n",
       "      <td>屏幕 音效 整体 效果 挺 轻薄 颜色 挺 特别 性能 没用 暂时 知道 买 最高 配置 应...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>2</td>\n",
       "      <td>手机 比较 发热 暂时 比较 流畅</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5942 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                  comment_processed\n",
       "0         4  一直 蓝厂 手机 妈妈 四年 该换 正好 刚出 配置 更好 更新 果断 入手 买 礼盒装 几...\n",
       "1         4        款 手机 颜色 太漂亮 玻璃 后盖绝 绝子 颜控 表示 喜欢 拍照 好看 电池 耐用 \n",
       "2         0  刚刚 买来 一个月 不到 电话 接听 不到 一天 几十个 电话 进来 手机 重要 功能 问题...\n",
       "3         4  运行 速度 真的 不要 太爽 使用 起来 不会 卡顿 之前 明显 不够 外形 外观 真的 很...\n",
       "4         0                               辣鸡 客服 刚买 手机 降价 补 差价 \n",
       "...     ...                                                ...\n",
       "5937      0                          最差 手机 说 手机 原来 手机 网络 差 要死 \n",
       "5938      4   拍照 效果 刚回来 试 一下 拍照 自带 美妆 挺不错 外形 外观 外观 好看 感觉 挺值 价 \n",
       "5939      4  外形 外观 外观 很漂亮 喜欢 屏幕 音效 生音 很大 拍照 效果 杠杠 运行 速度 很快 ...\n",
       "5940      2  屏幕 音效 整体 效果 挺 轻薄 颜色 挺 特别 性能 没用 暂时 知道 买 最高 配置 应...\n",
       "5941      2                                 手机 比较 发热 暂时 比较 流畅 \n",
       "\n",
       "[5942 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(train_data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f574aac-f218-4bc9-adf8-4a9b9ee8c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassificationModel, self).__init__()   \n",
    "        #加载预训练模型\n",
    "        pretrained_weights=\"Bert/bert-base-chinese/\"\n",
    "        self.bert = BertModel.from_pretrained(pretrained_weights)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        #定义线性函数      \n",
    "        self.dense = nn.Linear(768, 2)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "        \n",
    "    def forward(self, input_ids,token_type_ids,attention_mask):\n",
    "        #得到bert_output\n",
    "        bert_output = self.bert(input_ids=input_ids,token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        #获得预训练模型的输出\n",
    "        bert_cls_hidden_state = bert_output[1]\n",
    "        #将768维的向量输入到线性层映射为二维向量\n",
    "        linear_output = self.dense(bert_cls_hidden_state)\n",
    "        return  linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56162c35-d12e-42af-8414-f24f69a35d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(max_len,vocab_path,text_list):\n",
    "    #将text_list embedding成bert模型可用的输入形式\n",
    "    #加载分词模型\n",
    "    bert = BertModel.from_pretrained(\"Bert/bert-base-chinese/\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "    \n",
    "    tokenizer = tokenizer(\n",
    "        text_list,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = max_len,\n",
    "        return_tensors='pt'  # 返回的类型为pytorch tensor\n",
    "        )\n",
    "    input_ids = tokenizer['input_ids']\n",
    "    token_type_ids = tokenizer['token_type_ids']\n",
    "    attention_mask = tokenizer['attention_mask']\n",
    "    return input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9612c0d7-cd34-44e1-9701-0a51dee8e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    csvFileObj = open(path,encoding='UTF-8')\n",
    "    readerObj = csv.reader(csvFileObj)\n",
    "    text_list = []\n",
    "    labels = []\n",
    "    for row in readerObj:\n",
    "        #跳过表头\n",
    "        if readerObj.line_num == 1:\n",
    "            continue\n",
    "        #label在什么位置就改成对应的index\n",
    "        label = int(row[0])\n",
    "        text = row[1]\n",
    "        text_list.append(text)\n",
    "        labels.append(label)\n",
    "    #调用encoder函数，获得预训练模型的三种输入形式\n",
    "    input_ids,token_type_ids,attention_mask = encoder(max_len=150,vocab_path=\"Bert/vocab.txt\",text_list=text_list)\n",
    "    labels = torch.tensor(labels)\n",
    "    #将encoder的返回值以及label封装为Tensor的形式\n",
    "    data = TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893a3b17-db51-4ee6-b485-17925d3a136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1925: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#设定batch_size\n",
    "batch_size = 16\n",
    "#引入数据路径\n",
    "train_data_path=\"Train.csv\"\n",
    "dev_data_path=\"Dev.csv\"\n",
    "test_data_path=\"Test.csv\"\n",
    "#调用load_data函数，将数据加载为Tensor形式\n",
    "train_data = load_data(train_data_path)\n",
    "dev_data = load_data(dev_data_path)\n",
    "test_data = load_data(test_data_path)\n",
    "#将训练数据和测试数据进行DataLoader实例化\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dataset=dev_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e36d38-8151-4e04-a0e6-5cf77cd00d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(model,dev_loader):\n",
    "    #将模型放到服务器上\n",
    "    model.to(device)\n",
    "#设定模式为验证模式\n",
    "    model.eval()\n",
    "#设定不会有梯度的改变仅作验证\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for step, (input_ids,token_type_ids,attention_mask,labels) in tqdm(enumerate(dev_loader),desc='Dev Itreation:'):                \n",
    "            input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "            out_put = model(input_ids,token_type_ids,attention_mask)\n",
    "            _, predict = torch.max(out_put.data, 1)\n",
    "            correct += (predict==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        res = correct / total\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a092173d-5a65-469d-9bef-fec77772e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,dev_loader) :\n",
    "    #将model放到服务器上\n",
    "    model.to(device)\n",
    "    #设定模型的模式为训练模式\n",
    "    model.train()\n",
    "    #定义模型的损失函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    #设置模型参数的权重衰减\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    #学习率的设置\n",
    "    optimizer_params = {'lr': 1e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "    #使用AdamW 主流优化器\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, **optimizer_params)\n",
    "    #学习率调整器，检测准确率的状态，然后衰减学习率\n",
    "    scheduler = ReduceLROnPlateau(optimizer,mode='max',factor=0.5,min_lr=1e-7, patience=5,verbose= True, threshold=0.0001, eps=1e-08)\n",
    "    t_total = len(train_loader)\n",
    "    #设定训练轮次\n",
    "    total_epochs = 2\n",
    "    bestAcc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('Training and verification begin!')\n",
    "    for epoch in range(total_epochs): \n",
    "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(train_loader):\n",
    "#从实例化的DataLoader中取出数据，并通过 .to(device)将数据部署到服务器上    input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "            #梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            #将数据输入到模型中获得输出\n",
    "            out_put =  model(input_ids,token_type_ids,attention_mask)\n",
    "            #计算损失\n",
    "            loss = criterion(out_put, labels)\n",
    "            _, predict = torch.max(out_put.data, 1)\n",
    "            correct += (predict == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "             #每两步进行一次打印\n",
    "            if (step + 1) % 2 == 0:\n",
    "                train_acc = correct / total\n",
    "                print(\"Train Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,loss.item()))\n",
    "            #每五十次进行一次验证\n",
    "            if (step + 1) % 50 == 0:\n",
    "                train_acc = correct / total\n",
    "                #调用验证函数dev对模型进行验证，并将有效果提升的模型进行保存\n",
    "                acc = dev(model, dev_loader)\n",
    "                if bestAcc < acc:\n",
    "                    bestAcc = acc\n",
    "                    #模型保存路径\n",
    "                    path = 'savedmodel/span_bert_hide_model1.pkl'\n",
    "                    torch.save(model, path)\n",
    "                print(\"DEV Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,bestAcc{:.6f}%,dev_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,bestAcc*100,acc*100,loss.item()))\n",
    "        scheduler.step(bestAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8710bfd-871a-4262-946d-3ea3a3730f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and verification begin!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 4 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m BertClassificationModel()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#调用训练函数进行训练与验证\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, dev_loader)\u001b[0m\n\u001b[0;32m     35\u001b[0m out_put \u001b[38;5;241m=\u001b[39m  model(input_ids,token_type_ids,attention_mask)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#计算损失\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_put\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m _, predict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(out_put\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predict \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mD:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mD:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1047\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, Tensor)\n\u001b[1;32m-> 1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\torch\\nn\\functional.py:2690\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2689\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MySoftware\\Anaconda3\\envs\\StudyHuggingFaceBertEnv\\lib\\site-packages\\torch\\nn\\functional.py:2385\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected input batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) to match target batch_size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m   2383\u001b[0m     )\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2385\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m   2387\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss2d(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 4 is out of bounds."
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#实例化模型\n",
    "model = BertClassificationModel()\n",
    "#调用训练函数进行训练与验证\n",
    "train(model,train_loader,dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90391821-cfff-4254-aa48-30286c199281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    predict_probs = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(test_loader): \n",
    "            input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "            out_put = model(input_ids,token_type_ids,attention_mask)\n",
    "           \n",
    "            _, predict = torch.max(out_put.data, 1)\n",
    " \n",
    "            pre_numpy = predict.cpu().numpy().tolist()\n",
    "            predicts.extend(pre_numpy)\n",
    "            probs = F.softmax(out_put).detach().cpu().numpy().tolist()\n",
    "            predict_probs.extend(probs)\n",
    " \n",
    "            correct += (predict==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        res = correct / total\n",
    "        print('predict_Accuracy : {} %'.format(100 * res))\n",
    "        #返回预测结果和预测的概率\n",
    "        return predicts,predict_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e3250-b812-453f-81d2-22963bf01383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引进训练好的模型进行测试\n",
    "path = 'savedmodel/span_bert_hide_model.pkl'\n",
    "Trained_model = torch.load(path)\n",
    "#predicts是预测的（0   1），predict_probs是概率值\n",
    "predicts,predict_probs = predict(Trained_model,dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ad078-1f61-430e-80d8-87af92851fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = sklearn.metrics.precision_score(y_true, y_pred, average=’binary’,sample_weight=None)\n",
    "R = sklearn.metrics.recall_score(y_true, y_pred, average=’binary’,sample_weight=None)\n",
    "F1 = sklearn.metrics.f1_score(y_true, y_pred,average=’binary’,sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0887468-03ce-4602-b62e-4516ab05a4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
