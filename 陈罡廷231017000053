学号： 231017000053
姓名： 陈罡廷
专业： 计算机应用技术
 作业项目：中文完形填空
一、项目简介
 例如如下一道填空题：
性价比很高的一家，也是我目前________ 满意的一家。
人类很容易就能猜出横线处应该填写“最”字，这样才能符合上下文的语义。自然语言虽然
复杂，但却有着明显的统计规律，而神经网络最擅长的就是找出统计规律。 本项目将尝试
通 过 ChnSentiCrop 数 据 集 介 绍 了 中 文 填 空 任 务 过 程 ， 使 用 预 训 练 语 言 模 型
bert-base-chinese 直接在测试，也简要介绍了模型训练流程。
二、准备数据集
 本文所使用的数据集是 ChnSentiCorp 数据集，这是一个情感分类数据集。
 
ChnSentiCorp 数据集数据样例
 文本 答案
我家小朋友说两只小老鼠好可爱[MASK],她非常喜欢看，就因为看了...... 哦
值得一看，书里提出的问题值得[MASK]考，说得不无道理。个人支持...... 思
不好，每篇文章都很短，看起来[MASK]不痛快，刚刚看个开头就结束了...... 很
餐厅很差，菜的种类水准都不行[MASK]酒店基本没有旅游配套服务...... 。
性价比很高的一家，也是我目前[MASK]满意的一家。门口就有便利...... 最
本文做法为将每句话截断为固定的 30 个词，同时将第 15 个词替换为[MASK], 模型任务为根
据上下文预测第 15 个词。
1. 实现代码
加载编码
def load_encode_tool(pretrained_model_name_or_path):
 token = BertTokenizer.from_pretrained(Path(f'{pretrained_model_name_or_path}'))
 return token
if __name__ == '__main__':
 # 测试编码工具
 pretrained_model_name_or_path = 
r'L:\20230713_HuggingFaceModel\bert-base-chinese'
 token = load_encode_tool(pretrained_model_name_or_path)
 print(token)
输出结果如下：
BertTokenizer(name_or_path='L:\20230713_HuggingFaceModel\bert-base-chin
ese', vocab_size=21128, model_max_length=1000000000000000019884624838656, 
is_fast=False, padding_side='right', truncation_side='right', 
special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': 
'[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, 
clean_up_tokenization_spaces=True)
测试编码如下所示：
if __name__ == '__main__':
 # 测试编码工具
 pretrained_model_name_or_path = 
r'L:\20230713_HuggingFaceModel\bert-base-chinese'
 token = load_encode_tool(pretrained_model_name_or_path)
 # 测试编码句子
 out = token.batch_encode_plus(
 batch_text_or_text_pairs=[('不是一切大树，', '都被风暴折断。'),('不
是一切种子，', '都找不到生根的土壤。')],
 truncation=True,
 padding='max_length',
 max_length=18,
 return_tensors='pt',
 return_length=True, # 返回长度
 )
 # 查看编码输出
 for k, v in out.items():
 print(k, v.shape)
 print(token.decode(out['input_ids'][0]))
 print(token.decode(out['input_ids'][1]))
输出结果
input_ids torch.Size([2, 18])
token_type_ids torch.Size([2, 18])
length torch.Size([2])
attention_mask torch.Size([2, 18])
[CLS] 不 是 一 切 大 树 ， [SEP] 都 被 风 暴 折 断 。 [SEP] [PAD]
[CLS] 不 是 一 切 种 子 ， [SEP] 都 找 不 到 生 根 的 土 [SEP]
编码结果如下所示：
2. 定义数据集
def load_dataset_from_disk():
 pretrained_model_name_or_path = 
r'L:\20230713_HuggingFaceModel\ChnSentiCorp'
 dataset = load_from_disk(pretrained_model_name_or_path)
 # batched=True 表示批量处理
 # batch_size=1000 表示每次处理 1000 个样本
 # num_proc=8 表示使用 8 个线程操作
 # remove_columns=['text']表示移除 text 列
 dataset = dataset.map(f1, batched=True, batch_size=1000, num_proc=8, 
remove_columns=['text', 'label'])
 return dataset
if __name__ == '__main__':
 # 加载数据集
 dataset = load_dataset_from_disk()
 print(dataset)
结果输出如下所示
DatasetDict({
 train: Dataset({
 features: ['input_ids', 'token_type_ids', 'attention_mask', 
'length'],
 num_rows: 9600
 })
 validation: Dataset({
 features: ['input_ids', 'token_type_ids', 'attention_mask', 
'length'],
 num_rows: 1200
 })
 test: Dataset({
 features: ['input_ids', 'token_type_ids', 'attention_mask', 
'length'],
 num_rows: 1200
 })
})
3. 定义计算设备
对于大多数的神经网络计算来讲，在CUDA计算平台上进行计算比在CPU上要快。
# 定义计算设备
device = 'cpu'
if torch.cuda.is_available():
 device = 'CUDA'
# print(device)
4. 定义数据整理函数
本项目为填空任务， 现在的数据中每句话都是由 30 个词组成的，所以把每句话的第 15
个词挖出作为 label，也就是网络模型预测的目标，为了防止网络直接从原句子中读取答案，
把每句话的第 15 个词替换为[MASK].相当于在需要网络模型填答案的位置画横线，同时擦除
正确答案。网络模型需要根据[MASK]的上下文把[MASK]处原本的词预测出来。
 
数据整理函数的代码如下：
# 数据整理函数
def collate_fn(data):
 # 取出编码结果
 input_ids = [i['input_ids'] for i in data]
 attention_mask = [i['attention_mask'] for i in data]
 token_type_ids = [i['token_type_ids'] for i in data]
 # 转换为 Tensor 格式
 input_ids = torch.LongTensor(input_ids)
 attention_mask = torch.LongTensor(attention_mask)
 token_type_ids = torch.LongTensor(token_type_ids)
 # 把第 15 个词替换为 MASK
 labels = input_ids[:, 15].reshape(-1).clone()
 input_ids[:, 15] = token.get_vocab()[token.mask_token]
 # 移动到计算设备
 input_ids = input_ids.to(device)
 attention_mask = attention_mask.to(device)
 token_type_ids = token_type_ids.to(device)
 labels = labels.to(device)
 return input_ids, attention_mask, token_type_ids, labels
5. 定义数据集加载器
 定义了数据集和数据整理函数之后，可以定义数据集加载器，它能使用数据整
理函数来成批地处理数据集中的数据，代码如下：
# 数据集加载器
loader = torch.utils.data.DataLoader(dataset=dataset['train'], 
batch_size=16, collate_fn=collate_fn, shuffle=True, drop_last=True)
print(len(loader)) #600=9600/16
运行结果如下：
600 
可见训练数据集加载器一共加载了 580 个批次。
查看样例数据如下所示：
# 查看数据样例
for i, (input_ids, attention_mask, token_type_ids, labels) in 
enumerate(loader):
 break
print(input_ids.shape, attention_mask.shape, token_type_ids.shape, 
labels)
输出结果如下所示：
torch.Size([16, 30])
torch.Size([16, 30])
torch.Size([16, 30])
tensor([4638, 8024, 3198, 6206, 6392, 4761, 3449, 2128, 3341, 119, 3315, 
2697,
 2523, 2769, 6814, 1086], device='cuda:0')
三、定义模型：
1. 加载预训练模型
 完成以上准备工作，现在数据的结构已经准备好，可以输入模型进行计算了，即可加载
预训练模型了，代码如下：
pretrained_model_name_or_path = 
r'F:\20231209_HuggingFaceModel\bert-base-chinese'
# 加载预训练模型
pretrained = 
BertModel.from_pretrained(Path(f'{pretrained_model_name_or_path}'))
pretrained.to(device) 
定义后预训练模型之后，可以进行一次试算，代码如下：
#设定计算设备
Pretrained.to(device)
#模型试算
out = pretrained(input_ids=input_ids,
attentiion_mask=attention_mask,
Token_type_ids=token_type_ids) =
out.last_hidden_state.shape
运行结果如下：
torch.Size (16, 30, 768) 
2. 定义下游任务模型
下游任务模型将 BERT 提取第 15 个词的特征（16×768），输入到全连接神经网络（768
×21128），得到 16×21128，即把第 15 个词的特征投影到全体词表空间中，还原为词典
中的某个词。
class Model(torch.nn.Module):
 def __init__(self):
 super().__init__()
 self.decoder = torch.nn.Linear(in_features=768, 
out_features=token.vocab_size, bias=False)
 # 重新将 decode 中的 bias 参数初始化为全 o
 self.bias = 
torch.nn.Parameter(data=torch.zeros(token.vocab_size))
 self.decoder.bias = self.bias
 # 定义 Dropout 层，防止过拟合
 self.Dropout = torch.nn.Dropout(p=0.5)
 def forward(self, input_ids, attention_mask, token_type_ids):
 # 使用预训练模型抽取数据特征
 with torch.no_grad():
 out = pretrained(input_ids=input_ids, 
attention_mask=attention_mask, token_type_ids=token_type_ids)
 # 把第 15 个词的特征投影到全字典范围内
 out = self.Dropout(out.last_hidden_state[:, 15])
 out = self.decoder(out)
 return out
四、训练和测试
1. 训练
定义了 AdamW 优化器、loss 损失函数（交叉损失函数）和线性学习率调节器，如下所示
def train():
 # 定义优化器
 optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=1.0)
 # 定义 1oss 函数
 criterion = torch.nn.CrossEntropyLoss()
 # 定义学习率调节器
 scheduler = get_scheduler(name='linear', num_warmup_steps=0, 
num_training_steps=len(loader) * 5, optimizer=optimizer)
 # 将模型切换到训练模式
 model.train()
 # 共训练 5 个 epoch
 for epoch in range(5):
 # 按批次遍历训练集中的数据
 for i, (input_ids, attention_mask, token_type_ids, labels) in 
enumerate(loader):
 # 模型计算
 out = model(input_ids=input_ids, attention_mask=attention_mask, 
token_type_ids=token_type_ids)
 # 计算 loss 并使用梯度下降法优化模型参数
 loss = criterion(out, labels)
 loss.backward()
 optimizer.step()
 scheduler.step()
 optimizer.zero_grad()
 # 输出各项数据的情况，便于观察
if i % 50 == 0:
 out = out.argmax(dim=1)
 accuracy = (out == labels).sum().item() / len(labels)
 lr = optimizer.state_dict()['param_groups'][0]['lr']
 print(epoch, 1, loss.item(), lr, accuracy)
输出部分结果如下所示：
0 1 10.123428344726562 0.0004998333333333334 0.0
0 1 8.659417152404785 0.0004915 0.0625
0 1 7.431852340698242 0.0004831666666666667 0.0625
0 1 7.261701583862305 0.00047483333333333335 0.0625
0 1 6.693362236022949 0.0004665 0.125
0 1 4.0811614990234375 0.00045816666666666667 0.375
0 1 7.034963607788086 0.00044983333333333334 0.1875
2. 测试
使用测试数据集进行测试，如下所示
def test():
 # 定义测试数据集加载器
 loader_test = torch.utils.data.DataLoader(dataset=dataset['test'], 
batch_size=32, collate_fn=collate_fn, shuffle=True, drop_last=True)
 # 将下游任务模型切换到运行模式
 model.eval()
 correct = 0
 total = 0
 # 按批次遍历测试集中的数据
 for i, (input_ids, attention_mask, token_type_ids, labels) in 
enumerate(loader_test):
 # 计算 15 个批次即可，不需要全部遍历
 if i == 15:
 break
 print(i)
 # 计算
 with torch.no_grad():
 out = model(input_ids=input_ids, attention_mask=attention_mask, 
token_type_ids=token_type_ids)
 # 统计正确率
 out = out.argmax(dim=1)
 correct += (out == labels).sum().item()
 total += len(labels)
 print(correct / total)
在这段估码中，首先定义了测试数据集和加载器，并取出 5 个批次的数据让模型进行预测，
最后统计正确率并输出，运行结果如下：
0.56458333333333333 
五、小结
 本项目通过一个中文填空的例子讲解了使用 BERT 预训练模型抽取文本特征数据的方法，
事实上填空任务也是 BERT 模型本身在训练时的一个子任务，所以使用 BERT 模型在做填空任
务时效果往往较好，在处理不同的任务时，应该选择合适的预训练模型
